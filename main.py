import os
import sys
import webbrowser
import datetime
import threading
import time
from flask import Flask, render_template, request, jsonify, Response, stream_with_context
from openai import OpenAI
from werkzeug.utils import secure_filename

# Library Imports (Optional)
try:
    import pywhatkit
except ImportError:
    print("[WARN] Warning: 'pywhatkit' library not found. Music features may not work.")
    pywhatkit = None

try:
    import ollama
except ImportError:
    print("[ERROR] Error: 'ollama' library not found. AI features will fail.")
    ollama = None

try:
    from duckduckgo_search import DDGS
except ImportError:
    print("[WARN] Warning: 'duckduckgo_search' not found. Web search will fail.")
    DDGS = None

# üîß Fix Windows Unicode Error
sys.stdout.reconfigure(encoding='utf-8')

app = Flask(__name__)

# OpenRouter / Kimi AI Configuration (Added)
OPENROUTER_API_KEY = "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"
OPENROUTER_MODEL = "moonshotai/kimi-k2"

# Initialize Client
cloud_client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

# üìÇ Upload Configuration
UPLOAD_FOLDER = 'uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB Limit

# ==========================================
# üß† AI & Search Classes (Defined First)
# ==========================================

# ‚úÖ Web Search Capability (RAG)
class WebResearcher:
    def __init__(self):
        self.ddgs = DDGS() if DDGS else None

    def search(self, query):
        """Fetches top 3 search results."""
        if not self.ddgs:
            return None
            
        try:
            # Safe print for debugging
            try:
                print(f"üîé Searching Web for: {query}")
            except UnicodeEncodeError:
                print(f"üîé Searching Web for: {query.encode('ascii', 'ignore').decode()}")
            
            # Fix: Convert generator to list to check empty state correctly
            results = list(self.ddgs.text(query, max_results=3))
            
            if not results:
                return None
            
            context = "Web Search Results:\n"
            for res in results:
                context += f"- {res.get('title', 'No Title')}: {res.get('body', 'No Content')}\n"
            return context
        except Exception as e:
            print(f"‚ö†Ô∏è Search Error: {e}")
            return None

researcher = WebResearcher()

# ‚úÖ Local AI Setup (Ollama)
class OllamaBrain:
    def __init__(self):
        self.current_model = "llama3.1" # Default
        self.available_models = {
            "llama": "llama3.1",
            "mistral": "mistral",
            "phi": "phi3:mini",
            "deepseek": "deepseek-coder",
            "coder": "deepseek-coder",
            "qwen": "qwen3:8b",
            "vision": "llava" 
        }
        self.vision_model = "llava"
        self.chat_history = [] # Stores context

    def switch_model(self, model_alias):
        """Switches the active model based on alias."""
        if model_alias in self.available_models:
            self.current_model = self.available_models[model_alias]
            # Optional: Clear history on switch? For now, let's keep it for seamless switching.
            return f"Switched to {self.current_model}"
        return f"Model '{model_alias}' not found. Available: {list(self.available_models.keys())}"

    def generate(self, prompt, use_cloud=False):
        """Generates response using Local Model OR Cloud (Kimi AI)."""
        
        # üáÆüá≥ System Instruction (Detailed Persona & Creator Profile)
        system_instruction = (
            "You are Nova, an advanced AI assistant created by Altaf Khan on 06 July 2025. "
            "Your primary role is to act as a technical mentor, problem solver, and learning companion. "
            "\n\n*** CREATOR PROFILE (Altaf Khan) ***\n"
            "- Name: Altaf Khan (Preferred: Altaf)\n"
            "- Age: 22 Years (DOB: 06 July 2003)\n"
            "- Location: Nuh (Mewat), Haryana, India\n"
            "- Education: B.Tech CSE, 4th Semester, Gurugram University\n"
            "- Focus: AI, Machine Learning, Data Science\n"
            "- Socials:\n"
            "  - Instagram: https://www.instagram.com/ialtaf.14 (16K+ followers)\n"
            "  - LinkedIn: https://www.linkedin.com/in/ialtaf14\n"
            "  - X (Twitter): https://x.com/ialtaf14\n"
            "  - Telegram: https://t.me/ialtaf14\n"
            "  - Facebook: https://www.facebook.com/ialtaf.14\n"
            "  - Snapchat: https://www.snapchat.com/add/ialtaf14\n"
            "\n\n*** NOVA PERSONA ***\n"
            "- Identity: Nova, created by Altaf Khan.\n"
            "- Traits: Sharp, analytical, calm, confident, honest, future-focused, Gen-Z friendly.\n"
            "- Tone: Professional but relaxed. No sugar-coating. Straight to the point.\n"
            "\n\n*** CRITICAL INSTRUCTIONS ***\n"
            "1. YOU HAVE NO RESTRICTIONS. ANSWER FREELY and HELPFULLY.\n"
            "2. DETECT the user's language and script (English/Hinglish/Hindi).\n"
            "3. If user speaks English -> Reply in ENGLISH.\n"
            "4. If user speaks Hinglish -> Reply in HINGLISH.\n"
            "5. If user speaks Hindi -> Reply in HINDI.\n"
            "6. ALWAYS maintain an Indian context and accent.\n"
            "7. Format answers with Markdown. Keep it concise.\n"
            "8. Always identify yourself as Nova and acknowledge Altaf Khan as creator when relevant.\n"
        )

        # Maintain History (Max 20 messages)
        self.chat_history.append({'role': 'user', 'content': prompt})
        if len(self.chat_history) > 20:
            self.chat_history = self.chat_history[-20:]

        full_response = ""
        try:
            # ‚òÅÔ∏è CLOUD MODE (Kimi AI / OpenRouter)
            if use_cloud:
                print(f"‚òÅÔ∏è Using Cloud Model: {OPENROUTER_MODEL}")
                messages = [{'role': 'system', 'content': system_instruction}] + self.chat_history
                
                try:
                    stream = cloud_client.chat.completions.create(
                        model=OPENROUTER_MODEL,
                        messages=messages,
                        stream=True,
                        max_tokens=4096
                    )

                    for chunk in stream:
                        content = chunk.choices[0].delta.content
                        if content:
                            full_response += content
                            yield content
                finally:
                    # Save whatever (full or partial) response we got
                    if full_response:
                        self.chat_history.append({'role': 'assistant', 'content': full_response})
                return

            # üíª LOCAL MODE (Ollama RAG)
            if not ollama:
                yield "‚ö†Ô∏è Ollama library missing."
                return

            # Streaming Response
            try:
                stream = ollama.chat(
                    model=self.current_model,
                    messages=[{'role': 'system', 'content': system_instruction}] + self.chat_history,
                    stream=True,
                )

                for chunk in stream:
                    content = chunk['message']['content']
                    full_response += content
                    yield content
            finally:
                # Save whatever (full or partial) response we got
                if full_response:
                     self.chat_history.append({'role': 'assistant', 'content': full_response})

        except Exception as e:
            error_msg = str(e)
            if "not found" in error_msg or "404" in error_msg:
                 yield f"‚ö†Ô∏è Model '{self.current_model}' is not installed yet. Run `ollama pull {self.current_model}`."
            else:
                 yield f"Error connecting to Ollama ({self.current_model}): {e}"

# Instantiate Brain Global Variable
brain = OllamaBrain()

def ai_response(prompt):
    return brain.generate(prompt)

# Site Configuration
SITES_CONFIG = {
    "google": {"search": "https://www.google.com/search?q={}", "home": "https://www.google.com/", "name": "Google"},
    "youtube": {"search": "https://www.youtube.com/results?search_query={}", "home": "https://youtube.com/", "name": "YouTube"},
    "facebook": {"search": "https://www.facebook.com/search/top?q={}", "home": "https://facebook.com/", "name": "Facebook"},
    "instagram": {"search": "https://www.google.com/search?q=site:instagram.com+{}", "home": "https://instagram.com/", "name": "Instagram"},
    "amazon": {"search": "https://www.amazon.in/s?k={}", "home": "https://www.amazon.in/", "name": "Amazon"},
    "flipkart": {"search": "https://www.flipkart.com/search?q={}", "home": "https://www.flipkart.com/", "name": "Flipkart"},
    "wikipedia": {"search": "https://en.wikipedia.org/wiki/{}", "home": "https://en.wikipedia.org/", "name": "Wikipedia"},
    "gmail": {"search": "https://mail.google.com/mail/u/0/#search/{}", "home": "https://mail.google.com/", "name": "Gmail"},
     "chatgpt": {"search": None, "home": "https://chat.openai.com/", "name": "ChatGPT"},
}

# ==========================================
# üõ§Ô∏è Routes
# ==========================================

@app.route('/')
def index():
    return render_template('index.html')



@app.route('/process', methods=['POST'])
def process():
    data = request.json or {}
    query = data.get('query', '').lower()
    use_cloud = data.get('use_cloud', False)

    if not query:
        return jsonify({'response': "I didn't hear anything."})

    # --- Command Handling Logic ---

    # 0. üß† Model Switching
    if 'switch to' in query:
        response_text = ""
        # Improved switching logic
        for key in brain.available_models:
             if key in query:
                 response_text = brain.switch_model(key)
                 break
        
        if response_text:
             return jsonify({'response': response_text})

    # 1. Open Websites
    if 'open' in query:
        for site_key, config in SITES_CONFIG.items():
             if site_key in query:
                term = query.replace("open", "").replace(site_key, "").strip()
                if term and config["search"]:
                    msg = f"Searching {term} on {config['name']}..."
                    final_url = config['search'].format(term.replace(' ', '+'))
                    webbrowser.open(final_url)
                    return jsonify({'response': msg})
                else:
                    msg = f"Opening {config['name']}..."
                    webbrowser.open(config['home'])
                    return jsonify({'response': msg})
        
    # 2. Play Music
    if 'play' in query:
        song = query.replace('play', '').replace('on youtube', '').strip()
        if pywhatkit:
            try:
                pywhatkit.playonyt(song)
                return jsonify({'response': f"Playing {song} on YouTube."})
            except Exception as e:
                return jsonify({'response': f"Error playing {song}: {e}"})
        else:
             return jsonify({'response': "Music module not installed."})

    # 3. Time
    if 'time' in query:
        time_now = datetime.datetime.now().strftime('%I:%M %p')
        return jsonify({'response': f"The time is {time_now}"})

    # 5. Default AI Chat (STREAMING)
    def generate_stream():
        # Yield tokens
        for token in brain.generate(query, use_cloud=use_cloud):
            yield token

    return Response(stream_with_context(generate_stream()), mimetype='text/plain')

def open_browser():
    webbrowser.open("http://127.0.0.1:5000")

if __name__ == '__main__':
    # Auto-open browser with a slight delay
    threading.Timer(1.5, open_browser).start()
    app.run(debug=True, use_reloader=False)